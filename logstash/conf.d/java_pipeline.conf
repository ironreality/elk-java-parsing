# The idea is borrowed here - https://dzone.com/articles/using-multiple-grok-statements
# Prerequisites:
# 1. Current config was tested with Docker GELF log driver exclusively - https://docs.docker.com/config/containers/logging/gelf/
#
# 2. Logstash version: 6.x
#
# 3. Logstash multiline filter plugin installed - it's a stale plugin not installed by default after Logstash 2.4. 
#    https://github.com/logstash-plugins/logstash-filter-multiline

input {
  gelf {
    port => 5007
    type => "java"
  }
}

filter {
    if [type] == "java" {

# make some "normalization" of log fields with mutate filter plugin
        mutate {
            add_field => { "service_name" => "%{container_name}"
                            "stack_name" => "%{container_name}" 
            }
        }

        mutate {
            gsub => [ 
                "image_name", ".*\/", "",
                "image_name", "\:.*", "",
                "source_host", "\.", "-",
                "service_name", "\..*", "",
                "service_name", ".*_", "",
                "stack_name", "_.*", ""
            ]
        }

# cut out multiline Java traces from the message stream
        multiline {
            pattern => "(\[.+|^)%{TIMESTAMP_ISO8601}"
            negate => true
            what => "previous"
            source => "message"
            stream_identity => "%{host}.%{container_id}"
        }

# multiline message parsing
      if "multiline" in [tags] {
        grok {
            match => [ "message", "^(?<exception>[^:]+Exception):" ]
        }

			  # if an exception was parsed out - try to find it cause additionally 
        if "_grokparsefailure" not in [tags] {
            grok {
                match => [ "message", "(?m)^Caused by:(?<causedby>[^:]+Exception):" ]
            }
        }
      }
}
}

output {
    if [type] == "java" {
      elasticsearch {
        hosts => "localhost:9200"
      }
    }
}
